{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c58cf31-091c-462c-a2fa-320a49212f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import argparse\n",
    "\n",
    "import sys\n",
    "sys.path.append('../.')\n",
    "from utils.load_util import load_sdxl_models, load_pipe\n",
    "\n",
    "\n",
    "\n",
    "distillation_type='dmd' # what type of distillation model do you want to use (\"dmd\", \"lcm\", \"turbo\", \"lightning\")\n",
    "device = 'cuda:0'\n",
    "weights_dtype = torch.bfloat16\n",
    "\n",
    "pipe, base_unet, base_scheduler, distilled_unet, distilled_scheduler = load_sdxl_models(distillation_type=distillation_type, \n",
    "                                                                                        weights_dtype=weights_dtype, \n",
    "                                                                                        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25b0b8e-5e96-4c91-b427-83a2c2564215",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_guidance_scale= 5\n",
    "distilled_guidance_scale = 0\n",
    "\n",
    "run_base_till_timestep = None # set to none if you want it to be automatically decided\n",
    "run_distilled_from_timestep = 1\n",
    "\n",
    "\n",
    "# how many total timesteps to set for schedulers\n",
    "base_num_inference_steps = 4 \n",
    "distilled_num_inference_steps = 4\n",
    "\n",
    "# for paper consistent results use this\n",
    "base_scheduler = distilled_scheduler\n",
    "\n",
    "# set the timesteps for the model\n",
    "base_scheduler.set_timesteps(base_num_inference_steps)\n",
    "distilled_scheduler.set_timesteps(distilled_num_inference_steps)\n",
    "\n",
    "# automatically figure out what is the natural point to turn off the base model\n",
    "if run_base_till_timestep is None:\n",
    "    # check the timestep from which you need to run the model\n",
    "    distilled_timestep = distilled_scheduler.timesteps[run_distilled_from_timestep]\n",
    "\n",
    "    # check the closest timestep in basemodel\n",
    "    base_timesteps = abs(base_scheduler.timesteps - distilled_timestep)\n",
    "    run_base_till_timestep = base_timesteps.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488954db-b261-48b4-8bf1-47b70acb5d09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# User prompt\n",
    "prompt = \"cartoon character\"  # Replace with your desired prompt\n",
    "num_images = 15  # 5x5 grid\n",
    "\n",
    "\n",
    "nrows = 3\n",
    "ncols = 5\n",
    "\n",
    "# Initialize variables\n",
    "total_time = 0\n",
    "all_images = []\n",
    "\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "# Generate images\n",
    "for i in tqdm(range(num_images)):\n",
    "    # Generate random seed\n",
    "    seed = np.random.randint(0, 2**32 - 1)\n",
    "    generator = torch.manual_seed(seed)\n",
    "    \n",
    "    # First use base model\n",
    "    pipe.unet = base_unet\n",
    "    pipe.scheduler = base_scheduler\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    base_latents = pipe(prompt=prompt, from_timestep=0, till_timestep=run_base_till_timestep, \n",
    "                         guidance_scale=base_guidance_scale, num_inference_steps=base_num_inference_steps, \n",
    "                         output_type='latent')\n",
    "    \n",
    "    # Switch to distilled model\n",
    "    pipe.unet = distilled_unet\n",
    "    pipe.scheduler = distilled_scheduler\n",
    "    \n",
    "    \n",
    "    pil_image = pipe(prompt=prompt, start_latents=base_latents, guidance_scale=distilled_guidance_scale,\n",
    "                      from_timestep=run_distilled_from_timestep, till_timestep=None, \n",
    "                      num_inference_steps=distilled_num_inference_steps)[0]\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    runtime = end_time - start_time\n",
    "    total_time += runtime\n",
    "    \n",
    "    # Convert PIL image to numpy array and append to list\n",
    "    all_images.append(pil_image)\n",
    "\n",
    "# Create 5x5 grid with no whitespace\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*3, nrows*3), dpi=200)\n",
    "plt.subplots_adjust(wspace=0, hspace=0)  # Remove spacing between subplots\n",
    "fig.patch.set_visible(False)  # Hide the figure's background\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(all_images[i])\n",
    "    ax.axis('off')  # Remove axes\n",
    "    ax.set_xticklabels([])  # Remove tick labels\n",
    "    ax.set_yticklabels([])\n",
    "    ax.tick_params(left=False, bottom=False)  # Remove ticks\n",
    "\n",
    "# Remove any remaining borders/margins\n",
    "plt.tight_layout(pad=0, h_pad=0, w_pad=0)\n",
    "plt.subplots_adjust(left=0, right=1, top=1, bottom=0, wspace=0, hspace=0)\n",
    "plt.margins(0, 0)\n",
    "plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "print(f\"Total Runtime: {total_time:.4f} seconds\")\n",
    "print(f\"Single Runtime: {runtime:.4f} seconds\")\n",
    "# plt.savefig('grid.png', bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9772941-3c61-4fe2-8f48-5936c588df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for im in all_images:\n",
    "    display(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcfaa65-8d86-471d-8591-ad7924bde1c2",
   "metadata": {},
   "source": [
    "# Individual Pipe Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3784e391-7966-4234-8d97-08e0ee3e3dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import argparse\n",
    "\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from utils.load_util import load_sdxl_models, load_pipe\n",
    "\n",
    "\n",
    "\n",
    "distillation_type= 'dmd' # set to None for base model\n",
    "device = 'cuda:0'\n",
    "weights_dtype = torch.bfloat16\n",
    "\n",
    "pipe = load_pipe(distillation_type=distillation_type, \n",
    "                  weights_dtype=weights_dtype, \n",
    "                    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2545d82e-58a1-42c9-9bf6-51d4266fd49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_scale = 0\n",
    "num_inference_steps = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231963e1-b451-456e-b42a-5040d43c12c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# User prompt\n",
    "prompt = \"image of a dog\"  # Replace with your desired prompt\n",
    "num_images = 15  # 5x5 grid\n",
    "\n",
    "\n",
    "nrows = 5\n",
    "ncols = 3\n",
    "\n",
    "# Initialize variables\n",
    "total_time = 0\n",
    "all_images = []\n",
    "\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "# Generate images\n",
    "for i in tqdm(range(num_images)):\n",
    "    # Generate random seed\n",
    "    seed = np.random.randint(0, 2**32 - 1)\n",
    "    generator = torch.manual_seed(seed)\n",
    "    \n",
    "    # First use base model    \n",
    "    start_time = time.perf_counter()\n",
    "    pil_image = pipe(prompt=prompt, guidance_scale=guidance_scale,\n",
    "                      num_inference_steps=num_inference_steps)[0]\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    runtime = end_time - start_time\n",
    "    total_time += runtime\n",
    "    \n",
    "    # Convert PIL image to numpy array and append to list\n",
    "    all_images.append(pil_image)\n",
    "\n",
    "\n",
    "# Create 5x5 grid with no whitespace\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*3, nrows*3), dpi=200)\n",
    "plt.subplots_adjust(wspace=0, hspace=0)  # Remove spacing between subplots\n",
    "fig.patch.set_visible(False)  # Hide the figure's background\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(all_images[i])\n",
    "    ax.axis('off')  # Remove axes\n",
    "    ax.set_xticklabels([])  # Remove tick labels\n",
    "    ax.set_yticklabels([])\n",
    "    ax.tick_params(left=False, bottom=False)  # Remove ticks\n",
    "\n",
    "# Remove any remaining borders/margins\n",
    "plt.tight_layout(pad=0, h_pad=0, w_pad=0)\n",
    "plt.subplots_adjust(left=0, right=1, top=1, bottom=0, wspace=0, hspace=0)\n",
    "plt.margins(0, 0)\n",
    "plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "print(f\"Total Runtime: {total_time:.4f} seconds\")\n",
    "print(f\"Single Runtime: {runtime:.4f} seconds\")\n",
    "plt.savefig('grid.png', bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1dd75a-acfe-42f9-8db5-af6e321ea7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for im in all_images:\n",
    "    display(im)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592d567-b3e1-4e99-b86e-8334b54ad2e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
