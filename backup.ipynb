{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.enable_grad(False)\n",
        "\n",
        "# Enable PyTorch performance optimizations\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import argparse \n",
        "import random\n",
        "\n",
        "import sys\n",
        "sys.path.append('../.')\n",
        "from utils.load_util import load_sdxl_models, load_pipe\n",
        "\n",
        "\n",
        "\n",
        "distillation_type='dmd'  # what type of distillation model do you want to use (\"dmd\", \"lcm\", \"turbo\", \"lightning\")\n",
        "device = 'cuda:0'  # Use CUDA for A100 GPU\n",
        "weights_dtype = torch.bfloat16  # Use bfloat16 for better performance on A100\n",
        "\n",
        "pipe, base_unet, base_scheduler, distilled_unet, distilled_scheduler = load_sdxl_models(distillation_type=distillation_type, \n",
        "                                                                                        weights_dtype=weights_dtype, \n",
        "                                                                                        device=device)\n",
        "\n",
        "def diversity_distillation(prompt, seed, pipe, base_unet, distilled_unet, distilled_scheduler, base_guidance_scale=5, distilled_guidance_scale=0, num_inference_steps=4, run_base_till=1):\n",
        "    pipe.scheduler = distilled_scheduler\n",
        "    pipe.unet=base_unet\n",
        "\n",
        "    base_latents = pipe(prompt,\n",
        "                    guidance_scale=base_guidance_scale,\n",
        "                    till_timestep=run_base_till, \n",
        "                    num_inference_steps=num_inference_steps,\n",
        "                    generator=torch.Generator().manual_seed(seed),\n",
        "                    output_type='latent'\n",
        "                   )\n",
        "    \n",
        "\n",
        "    pipe.unet = distilled_unet\n",
        "    images = pipe(prompt,\n",
        "                 guidance_scale=distilled_guidance_scale,\n",
        "                 start_latents = base_latents,   \n",
        "                 num_inference_steps=num_inference_steps,\n",
        "                 from_timestep=run_base_till,\n",
        "                 output_type='pil'\n",
        "                )\n",
        "    return images    "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "positive_prompt = \"Close-up editorial photo of a linen blazer on a hanger, natural side lighting, visible weave texture, neutral colour grading, minimalistic background, depth and grain like high-end magazine\"\n",
        "\n",
        "negative_prompt = \"oversharpened, crispy edges, noise, plastic skin, watercolor effect, haloing, moiré\"\n",
        "\n",
        "\n",
        "\n",
        "seed = 42\n",
        "\n",
        "seed = 42\n",
        "from diffusers import UNet2DConditionModel, LCMScheduler, StableDiffusionXLPipeline, EulerAncestralDiscreteScheduler, EulerDiscreteScheduler, DDIMScheduler, TCDScheduler\n",
        "\n",
        "\n",
        "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.unet = base_unet\n",
        "\n",
        "image_no_neg = pipe(\n",
        "    positive_prompt,\n",
        "    guidance_scale=9,\n",
        "    num_inference_steps=100,\n",
        "    generator=torch.Generator(device=pipe.device).manual_seed(seed),\n",
        "    output_type=\"pil\"\n",
        ")[0]\n",
        "\n",
        "image_with_neg = pipe(\n",
        "    positive_prompt,\n",
        "    guidance_scale=9,\n",
        "    negative_prompt=negative_prompt,\n",
        "    num_inference_steps=100,\n",
        "    generator=torch.Generator(device=pipe.device).manual_seed(seed),\n",
        "    output_type=\"pil\"\n",
        ")[0]\n",
        "\n",
        "# Display side by side\n",
        "import matplotlib.pyplot as plt\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "ax1.imshow(image_no_neg)\n",
        "ax1.set_title(\"Without Negative Prompt\", fontsize=14, fontweight='bold')\n",
        "ax1.axis('off')\n",
        "\n",
        "ax2.imshow(image_with_neg)\n",
        "ax2.set_title(\"With Negative Prompt\", fontsize=14, fontweight='bold')\n",
        "ax2.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✅ Comparison complete!\")\n",
        "print(f\"Look for differences: the negative prompt should reduce presence of '{negative_prompt}'\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#positive_prompt = \"Close-up editorial photo of a linen blazer on a hanger, natural side lighting, visible weave texture, neutral colour grading, minimalistic background, depth and grain like high-end magazine\"\n",
        "\n",
        "#negative_prompt = \"oversharpened, crispy edges, noise, plastic skin, watercolor effect, haloing, moiré\"\n",
        "\n",
        "positive_prompt = \"A single realistic 3d cube with detailed wood grain pattern\"\n",
        "\n",
        "#negative_prompt = \"oversharpened, crispy edges, noise, plastic skin, watercolor effect, haloing, moiré\"\n",
        "\n",
        "\n",
        "seed = 31\n",
        "\n",
        "distillation_type='turbo'  # what type of distillation model do you want to use (\"dmd\", \"lcm\", \"turbo\", \"lightning\")\n",
        "device = 'cuda:0'  # Use CUDA for A100 GPU\n",
        "weights_dtype = torch.bfloat16  # Use bfloat16 for better performance on A100\n",
        "\n",
        "pipe, base_unet, base_scheduler, distilled_unet, distilled_scheduler = load_sdxl_models(distillation_type=distillation_type, \n",
        "                                                                                        weights_dtype=weights_dtype, \n",
        "                                                                                        device=device)\n",
        "\n",
        "pipe.scheduler = distilled_scheduler\n",
        "pipe.unet=base_unet\n",
        "\n",
        "base_latents = pipe(positive_prompt,\n",
        "                guidance_scale=7,\n",
        "#                negative_prompt=negative_prompt,\n",
        "                till_timestep=1, \n",
        "                num_inference_steps=4,\n",
        "                generator=torch.Generator().manual_seed(seed),\n",
        "                output_type='latent'\n",
        "                )\n",
        "\n",
        "\n",
        "pipe.unet = distilled_unet\n",
        "images = pipe(positive_prompt,\n",
        "                guidance_scale=0,\n",
        "                start_latents = base_latents,  \n",
        "                num_inference_steps=4,\n",
        "                from_timestep=1,\n",
        "                output_type='pil'\n",
        "            )\n",
        "\n",
        "\n",
        "images[0]\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Distilled Model Only - Simple Generation\n",
        "#positive_prompt = \"Close-up editorial photo of a linen blazer on a hanger, natural side lighting, visible weave texture, neutral colour grading, minimalistic background, depth and grain like high-end magazine\"\n",
        "\n",
        "#negative_prompt = \"oversharpened, crispy edges, noise, plastic skin, watercolor effect, haloing, moiré\"\n",
        "positive_prompt = \"A single realistic 3d cube with detailed wood grain pattern\"\n",
        "\n",
        "seed = 31\n",
        "\n",
        "distillation_type = 'turbo'\n",
        "device = 'cuda:0'\n",
        "weights_dtype = torch.bfloat16\n",
        "\n",
        "pipe, base_unet, base_scheduler, distilled_unet, distilled_scheduler = load_sdxl_models(\n",
        "    distillation_type=distillation_type,\n",
        "    weights_dtype=weights_dtype,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Use only the distilled model\n",
        "pipe.scheduler = distilled_scheduler\n",
        "pipe.unet = distilled_unet\n",
        "\n",
        "images = pipe(\n",
        "    positive_prompt,\n",
        "    guidance_scale=1,\n",
        "    negative_prompt=negative_prompt,\n",
        "    num_inference_steps=4,\n",
        "    generator=torch.Generator().manual_seed(seed),\n",
        "    output_type='pil'\n",
        ")\n",
        "\n",
        "images[0]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple SDXL Base Test\n",
        "prompt = \"natural light editorial headshot of a 28-year-old woman, half-body, slight smile, shallow depth of field, photorealistic, balanced colours, red hair\"\n",
        "seed = 42\n",
        "from diffusers import UNet2DConditionModel, LCMScheduler, StableDiffusionXLPipeline, EulerAncestralDiscreteScheduler, EulerDiscreteScheduler, DDIMScheduler, TCDScheduler\n",
        "\n",
        "\n",
        "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.unet = base_unet\n",
        "\n",
        "image = pipe(\n",
        "    prompt,\n",
        "    guidance_scale=5.0,\n",
        "    num_inference_steps=100,\n",
        "    generator=torch.Generator(device=pipe.device).manual_seed(seed),\n",
        "    output_type=\"pil\",\n",
        ")[0]\n",
        "\n",
        "image"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Function to generate images using only the distilled model\n",
        "def distilled_only_generation(prompt, seed, pipe, distilled_unet, distilled_scheduler, guidance_scale=0, num_inference_steps=4):\n",
        "    pipe.scheduler = distilled_scheduler\n",
        "    pipe.unet = distilled_unet\n",
        "    \n",
        "    images = pipe(prompt,\n",
        "                 guidance_scale=guidance_scale,\n",
        "                 num_inference_steps=num_inference_steps,\n",
        "                 generator=torch.Generator().manual_seed(seed),\n",
        "                 output_type='pil'\n",
        "                )\n",
        "    return images\n",
        "\n",
        "# Function to generate images using only the base model\n",
        "def base_only_generation(prompt, seed, pipe, base_unet, base_scheduler, guidance_scale=5, num_inference_steps=20):\n",
        "    pipe.scheduler = base_scheduler\n",
        "    pipe.unet = base_unet\n",
        "    \n",
        "    images = pipe(prompt,\n",
        "                 guidance_scale=guidance_scale,\n",
        "                 num_inference_steps=num_inference_steps,\n",
        "                 generator=torch.Generator().manual_seed(seed),\n",
        "                 output_type='pil'\n",
        "                )\n",
        "    return images\n",
        "\n",
        "# User prompt\n",
        "prompt = \"bear in a top hat\"  # Replace with your desired prompt\n",
        "num_images = 6  # 2x3 grid\n",
        "\n",
        "# Create output directory\n",
        "output_dir = \"generated_images\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "nrows = 2\n",
        "ncols = 3\n",
        "\n",
        "# Initialize variables\n",
        "total_time_diversity = 0\n",
        "total_time_distilled = 0\n",
        "total_time_base = 0\n",
        "all_diversity_images = []\n",
        "all_distilled_images = []\n",
        "all_base_images = []\n",
        "\n",
        "pipe.set_progress_bar_config(disable=True)\n",
        "\n",
        "# Generate images with all three methods\n",
        "for i in tqdm(range(num_images)):\n",
        "    # Generate random seed (use same seed for all methods for fair comparison)\n",
        "    seed = np.random.randint(0, 2**32 - 1)\n",
        "    \n",
        "    # Generate diversity distillation image\n",
        "    start_time = time.perf_counter()\n",
        "    diversity_image = diversity_distillation(prompt, seed, pipe, base_unet, distilled_unet, distilled_scheduler)[0]\n",
        "    end_time = time.perf_counter()\n",
        "    runtime_diversity = end_time - start_time\n",
        "    total_time_diversity += runtime_diversity\n",
        "    \n",
        "    # Generate distilled-only image\n",
        "    start_time = time.perf_counter()\n",
        "    distilled_image = distilled_only_generation(prompt, seed, pipe, distilled_unet, distilled_scheduler)[0]\n",
        "    end_time = time.perf_counter()\n",
        "    runtime_distilled = end_time - start_time\n",
        "    total_time_distilled += runtime_distilled\n",
        "    \n",
        "    # Generate base-only image\n",
        "    start_time = time.perf_counter()\n",
        "    base_image = base_only_generation(prompt, seed, pipe, base_unet, base_scheduler)[0]\n",
        "    end_time = time.perf_counter()\n",
        "    runtime_base = end_time - start_time\n",
        "    total_time_base += runtime_base\n",
        "    \n",
        "    # Save individual images to disk\n",
        "    diversity_filename = f\"{output_dir}/diversity_image_{i+1:02d}_seed_{seed}.png\"\n",
        "    distilled_filename = f\"{output_dir}/distilled_image_{i+1:02d}_seed_{seed}.png\"\n",
        "    base_filename = f\"{output_dir}/base_image_{i+1:02d}_seed_{seed}.png\"\n",
        "    diversity_image.save(diversity_filename)\n",
        "    distilled_image.save(distilled_filename)\n",
        "    base_image.save(base_filename)\n",
        "    \n",
        "    # Append to lists for grid creation\n",
        "    all_diversity_images.append(diversity_image)\n",
        "    all_distilled_images.append(distilled_image)\n",
        "    all_base_images.append(base_image)\n",
        "\n",
        "# Create comparison figure with three subplots side by side\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(ncols*9, nrows*3), dpi=200)\n",
        "\n",
        "# Create grid for diversity distillation images\n",
        "ax1.set_title(\"Diversity Distillation\\n(Base + Distilled)\", fontsize=14, pad=20)\n",
        "for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "        idx = i * ncols + j\n",
        "        if idx < len(all_diversity_images):\n",
        "            # Calculate position for each image in the grid\n",
        "            y_start = (nrows - 1 - i) / nrows\n",
        "            y_end = (nrows - i) / nrows\n",
        "            x_start = j / ncols\n",
        "            x_end = (j + 1) / ncols\n",
        "            \n",
        "            # Create inset axes for each image\n",
        "            img_ax = ax1.inset_axes([x_start, y_start, x_end - x_start, y_end - y_start])\n",
        "            img_ax.imshow(all_diversity_images[idx])\n",
        "            img_ax.axis('off')\n",
        "\n",
        "ax1.set_xlim(0, 1)\n",
        "ax1.set_ylim(0, 1)\n",
        "ax1.axis('off')\n",
        "\n",
        "# Create grid for distilled-only images\n",
        "ax2.set_title(\"Distilled Model Only\\n(4 steps)\", fontsize=14, pad=20)\n",
        "for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "        idx = i * ncols + j\n",
        "        if idx < len(all_distilled_images):\n",
        "            # Calculate position for each image in the grid\n",
        "            y_start = (nrows - 1 - i) / nrows\n",
        "            y_end = (nrows - i) / nrows\n",
        "            x_start = j / ncols\n",
        "            x_end = (j + 1) / ncols\n",
        "            \n",
        "            # Create inset axes for each image\n",
        "            img_ax = ax2.inset_axes([x_start, y_start, x_end - x_start, y_end - y_start])\n",
        "            img_ax.imshow(all_distilled_images[idx])\n",
        "            img_ax.axis('off')\n",
        "\n",
        "ax2.set_xlim(0, 1)\n",
        "ax2.set_ylim(0, 1)\n",
        "ax2.axis('off')\n",
        "\n",
        "# Create grid for base-only images\n",
        "ax3.set_title(\"Base Model Only\\n(20 steps)\", fontsize=14, pad=20)\n",
        "for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "        idx = i * ncols + j\n",
        "        if idx < len(all_base_images):\n",
        "            # Calculate position for each image in the grid\n",
        "            y_start = (nrows - 1 - i) / nrows\n",
        "            y_end = (nrows - i) / nrows\n",
        "            x_start = j / ncols\n",
        "            x_end = (j + 1) / ncols\n",
        "            \n",
        "            # Create inset axes for each image\n",
        "            img_ax = ax3.inset_axes([x_start, y_start, x_end - x_start, y_end - y_start])\n",
        "            img_ax.imshow(all_base_images[idx])\n",
        "            img_ax.axis('off')\n",
        "\n",
        "ax3.set_xlim(0, 1)\n",
        "ax3.set_ylim(0, 1)\n",
        "ax3.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Print timing information\n",
        "print(f\"Diversity Distillation - Total Runtime: {total_time_diversity:.4f} seconds\")\n",
        "print(f\"Diversity Distillation - Average per image: {total_time_diversity/num_images:.4f} seconds\")\n",
        "print(f\"Distilled Only - Total Runtime: {total_time_distilled:.4f} seconds\")\n",
        "print(f\"Distilled Only - Average per image: {total_time_distilled/num_images:.4f} seconds\")\n",
        "print(f\"Base Only - Total Runtime: {total_time_base:.4f} seconds\")\n",
        "print(f\"Base Only - Average per image: {total_time_base/num_images:.4f} seconds\")\n",
        "print(f\"Individual images saved to: {output_dir}/\")\n",
        "\n",
        "# Save the comparison grid\n",
        "comparison_filename = f\"{output_dir}/three_way_comparison_{prompt.replace(' ', '_')}.png\"\n",
        "plt.savefig(comparison_filename, bbox_inches='tight', pad_inches=0.1, dpi=200)\n",
        "print(f\"Three-way comparison grid saved to: {comparison_filename}\")\n",
        "\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Configuration\n",
        "prompt = \"frog in a hat\"\n",
        "num_images = 6  # 2x3 grid\n",
        "nrows = 2\n",
        "ncols = 3\n",
        "\n",
        "# Models to compare\n",
        "model_types = ['dmd', 'lightning', 'turbo']\n",
        "model_names = {\n",
        "    'dmd': 'DMD',\n",
        "    'lightning': 'Lightning', \n",
        "    'turbo': 'Turbo'\n",
        "}\n",
        "\n",
        "# Create output directory\n",
        "output_dir = \"multi_model_comparison\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Store results for each model and method\n",
        "all_results = {}\n",
        "timing_results = {}\n",
        "\n",
        "print(f\"Comparing 3 generation methods across {len(model_types)} models: {', '.join(model_names.values())}\")\n",
        "print(f\"Generating {num_images} images per method per model with prompt: '{prompt}'\\n\")\n",
        "\n",
        "# Load and test each model\n",
        "for model_type in model_types:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Loading {model_names[model_type]} model...\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Load the model\n",
        "    start_load = time.time()\n",
        "    model_pipe, model_base_unet, model_base_scheduler, model_distilled_unet, model_distilled_scheduler = load_sdxl_models(\n",
        "        distillation_type=model_type,\n",
        "        weights_dtype=torch.bfloat16,\n",
        "        device=device\n",
        "    )\n",
        "    load_time = time.time() - start_load\n",
        "    print(f\"✓ {model_names[model_type]} loaded in {load_time:.2f}s\")\n",
        "    \n",
        "    # Initialize storage for this model\n",
        "    all_results[model_type] = {\n",
        "        'diversity': [],\n",
        "        'distilled': [],\n",
        "        'base': []\n",
        "    }\n",
        "    timing_results[model_type] = {\n",
        "        'diversity': {'total': 0, 'average': 0},\n",
        "        'distilled': {'total': 0, 'average': 0},\n",
        "        'base': {'total': 0, 'average': 0},\n",
        "        'load_time': load_time\n",
        "    }\n",
        "    \n",
        "    model_pipe.set_progress_bar_config(disable=True)\n",
        "    \n",
        "    # Generate images with all three methods\n",
        "    for i in tqdm(range(num_images), desc=f\"Generating {model_names[model_type]} images\"):\n",
        "        # Use same seed for all methods for fair comparison\n",
        "        seed = np.random.randint(0, 2**32 - 1)\n",
        "        \n",
        "        # 1. Diversity Distillation (Base + Distilled)\n",
        "        start_time = time.perf_counter()\n",
        "        diversity_images = diversity_distillation(\n",
        "            prompt, seed, model_pipe, \n",
        "            model_base_unet, model_distilled_unet, \n",
        "            model_distilled_scheduler\n",
        "        )\n",
        "        gen_time = time.perf_counter() - start_time\n",
        "        timing_results[model_type]['diversity']['total'] += gen_time\n",
        "        all_results[model_type]['diversity'].append(diversity_images[0])\n",
        "        \n",
        "        # Save individual image\n",
        "        diversity_filename = f\"{output_dir}/{model_type}_diversity_{i+1:02d}_seed_{seed}.png\"\n",
        "        diversity_images[0].save(diversity_filename)\n",
        "        \n",
        "        # 2. Distilled Model Only\n",
        "        start_time = time.perf_counter()\n",
        "        distilled_images = distilled_only_generation(\n",
        "            prompt, seed, model_pipe,\n",
        "            model_distilled_unet, model_distilled_scheduler\n",
        "        )\n",
        "        gen_time = time.perf_counter() - start_time\n",
        "        timing_results[model_type]['distilled']['total'] += gen_time\n",
        "        all_results[model_type]['distilled'].append(distilled_images[0])\n",
        "        \n",
        "        # Save individual image\n",
        "        distilled_filename = f\"{output_dir}/{model_type}_distilled_{i+1:02d}_seed_{seed}.png\"\n",
        "        distilled_images[0].save(distilled_filename)\n",
        "        \n",
        "        # 3. Base Model Only\n",
        "        start_time = time.perf_counter()\n",
        "        base_images = base_only_generation(\n",
        "            prompt, seed, model_pipe,\n",
        "            model_base_unet, model_base_scheduler\n",
        "        )\n",
        "        gen_time = time.perf_counter() - start_time\n",
        "        timing_results[model_type]['base']['total'] += gen_time\n",
        "        all_results[model_type]['base'].append(base_images[0])\n",
        "        \n",
        "        # Save individual image\n",
        "        base_filename = f\"{output_dir}/{model_type}_base_{i+1:02d}_seed_{seed}.png\"\n",
        "        base_images[0].save(base_filename)\n",
        "    \n",
        "    # Calculate averages\n",
        "    for method in ['diversity', 'distilled', 'base']:\n",
        "        timing_results[model_type][method]['average'] = timing_results[model_type][method]['total'] / num_images\n",
        "    \n",
        "    print(f\"✓ Generated {num_images*3} images total ({num_images} per method)\")\n",
        "    \n",
        "    # Clean up to free memory\n",
        "    del model_pipe, model_base_unet, model_base_scheduler, model_distilled_unet, model_distilled_scheduler\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Creating comparison visualization...\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# Create comparison figure: 3 rows (methods) x 3 columns (models)\n",
        "fig, axes = plt.subplots(3, 3, figsize=(ncols*10, nrows*11), dpi=200)\n",
        "\n",
        "method_labels = {\n",
        "    'diversity': 'Diversity Distillation\\n(Base + Distilled)',\n",
        "    'distilled': 'Distilled Model Only\\n(4 steps)',\n",
        "    'base': 'Base Model Only\\n(20 steps)'\n",
        "}\n",
        "methods = ['diversity', 'distilled', 'base']\n",
        "\n",
        "# Create grid for each combination of method and model\n",
        "for method_idx, method in enumerate(methods):\n",
        "    for model_idx, model_type in enumerate(model_types):\n",
        "        ax = axes[method_idx, model_idx]\n",
        "        model_name = model_names[model_type]\n",
        "        avg_time = timing_results[model_type][method]['average']\n",
        "        \n",
        "        # Title includes model name, method, and timing\n",
        "        if method_idx == 0:  # Top row gets model name\n",
        "            title = f\"{model_name}\\n{method_labels[method]}\\n{avg_time:.2f}s/img\"\n",
        "        else:\n",
        "            title = f\"{method_labels[method]}\\n{avg_time:.2f}s/img\"\n",
        "        \n",
        "        ax.set_title(title, fontsize=12, pad=15, fontweight='bold')\n",
        "        \n",
        "        # Create grid for this method's images\n",
        "        for i in range(nrows):\n",
        "            for j in range(ncols):\n",
        "                idx = i * ncols + j\n",
        "                if idx < len(all_results[model_type][method]):\n",
        "                    # Calculate position for each image in the grid\n",
        "                    y_start = (nrows - 1 - i) / nrows\n",
        "                    y_end = (nrows - i) / nrows\n",
        "                    x_start = j / ncols\n",
        "                    x_end = (j + 1) / ncols\n",
        "                    \n",
        "                    # Create inset axes for each image\n",
        "                    img_ax = ax.inset_axes([x_start, y_start, x_end - x_start, y_end - y_start])\n",
        "                    img_ax.imshow(all_results[model_type][method][idx])\n",
        "                    img_ax.axis('off')\n",
        "        \n",
        "        ax.set_xlim(0, 1)\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Print comprehensive timing information\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TIMING RESULTS\")\n",
        "print(\"=\"*60)\n",
        "for model_type in model_types:\n",
        "    model_name = model_names[model_type]\n",
        "    results = timing_results[model_type]\n",
        "    print(f\"\\n{model_name} (Load time: {results['load_time']:.2f}s):\")\n",
        "    print(f\"  Diversity Distillation:\")\n",
        "    print(f\"    Total: {results['diversity']['total']:.2f}s | Avg: {results['diversity']['average']:.2f}s | Throughput: {1.0/results['diversity']['average']:.2f} img/s\")\n",
        "    print(f\"  Distilled Only:\")\n",
        "    print(f\"    Total: {results['distilled']['total']:.2f}s | Avg: {results['distilled']['average']:.2f}s | Throughput: {1.0/results['distilled']['average']:.2f} img/s\")\n",
        "    print(f\"  Base Only:\")\n",
        "    print(f\"    Total: {results['base']['total']:.2f}s | Avg: {results['base']['average']:.2f}s | Throughput: {1.0/results['base']['average']:.2f} img/s\")\n",
        "\n",
        "print(f\"\\nIndividual images saved to: {output_dir}/\")\n",
        "\n",
        "# Save the comparison grid\n",
        "comparison_filename = f\"{output_dir}/full_comparison_{prompt.replace(' ', '_')}.png\"\n",
        "plt.savefig(comparison_filename, bbox_inches='tight', pad_inches=0.1, dpi=200)\n",
        "print(f\"Full comparison grid saved to: {comparison_filename}\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✅ Multi-model multi-method comparison complete!\")\n",
        "print(f\"Total images generated: {len(model_types) * 3 * num_images} ({len(model_types)} models × 3 methods × {num_images} images)\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Configuration - More compositional prompts for clearer negative testing\n",
        "positive_prompt = \"horse pulling a carriage\"\n",
        "negative_prompt = \"wheels\"  # Compositional: opposite objects/attributes\n",
        "seed = 42  # Fixed seed for consistency\n",
        "\n",
        "# CFG scales to test\n",
        "cfg_scales = [0.0, 2.5, 5.0, 7.5, 10.0]  # Test range from no guidance to high guidance\n",
        "\n",
        "# All 8 distillation models plus base\n",
        "model_types = ['base', 'dmd', 'turbo', 'lightning', 'lcm', 'hyper', 'pcm', 'tcd', 'flash']\n",
        "model_names = {\n",
        "    'base': 'Base SDXL',\n",
        "    'dmd': 'DMD2',\n",
        "    'turbo': 'SDXL-Turbo',\n",
        "    'lightning': 'SDXL-Lightning',\n",
        "    'lcm': 'LCM-LoRA',\n",
        "    'hyper': 'Hyper-SDXL',\n",
        "    'pcm': 'PCM',\n",
        "    'tcd': 'TCD',\n",
        "    'flash': 'Flash Diffusion'\n",
        "}\n",
        "\n",
        "# Recommended settings per model\n",
        "model_configs = {\n",
        "    'base': {'steps': 20, 'recommended_cfg': 5.0},\n",
        "    'dmd': {'steps': 4, 'recommended_cfg': 0.0},\n",
        "    'turbo': {'steps': 1, 'recommended_cfg': 0.0},\n",
        "    'lightning': {'steps': 4, 'recommended_cfg': 0.0},\n",
        "    'lcm': {'steps': 4, 'recommended_cfg': 1.0},\n",
        "    'hyper': {'steps': 8, 'recommended_cfg': 5.0},  # CFG-preserved version\n",
        "    'pcm': {'steps': 4, 'recommended_cfg': 2.0},\n",
        "    'tcd': {'steps': 4, 'recommended_cfg': 3.0},  # Supports standard CFG\n",
        "    'flash': {'steps': 4, 'recommended_cfg': 2.0}\n",
        "}\n",
        "\n",
        "output_dir = \"test-output\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Store results\n",
        "all_results = {\n",
        "    'cfg_sweep': {},  # CFG scale responses\n",
        "    'negative_test': {}  # Negative prompt effectiveness\n",
        "}\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"COMPREHENSIVE CFG & COMPOSITIONAL NEGATIVE PROMPT TESTING\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Positive Prompt: {positive_prompt}\")\n",
        "print(f\"Negative Prompt: {negative_prompt}\")\n",
        "print(f\"Seed: {seed}\")\n",
        "print(f\"Testing {len(model_types)} models\\n\")\n",
        "\n",
        "# Test each model\n",
        "for model_type in model_types:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Testing {model_names[model_type]} model...\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Load the model\n",
        "    if model_type == 'base':\n",
        "        # Base model doesn't need distillation loading\n",
        "        test_pipe, test_base_unet, test_base_scheduler, _, _ = load_sdxl_models(\n",
        "            distillation_type='dmd',  # Dummy, we'll use base components\n",
        "            weights_dtype=torch.bfloat16,\n",
        "            device=device\n",
        "        )\n",
        "        test_unet = test_base_unet\n",
        "        test_scheduler = test_base_scheduler\n",
        "    else:\n",
        "        test_pipe, test_base_unet, test_base_scheduler, test_distilled_unet, test_distilled_scheduler = load_sdxl_models(\n",
        "            distillation_type=model_type,\n",
        "            weights_dtype=torch.bfloat16,\n",
        "            device=device\n",
        "        )\n",
        "        test_unet = test_distilled_unet\n",
        "        test_scheduler = test_distilled_scheduler\n",
        "    \n",
        "    test_pipe.set_progress_bar_config(disable=True)\n",
        "    config = model_configs[model_type]\n",
        "    \n",
        "    all_results['cfg_sweep'][model_type] = []\n",
        "    all_results['negative_test'][model_type] = {}\n",
        "    \n",
        "    # Test 1: CFG Scale Sweep\n",
        "    print(f\"\\n1. Testing CFG scale response (recommended: {config['recommended_cfg']})...\")\n",
        "    for cfg in cfg_scales:\n",
        "        test_pipe.scheduler = test_scheduler\n",
        "        test_pipe.unet = test_unet\n",
        "        \n",
        "        image = test_pipe(\n",
        "            positive_prompt,\n",
        "            guidance_scale=cfg,\n",
        "            num_inference_steps=config['steps'],\n",
        "            generator=torch.Generator().manual_seed(seed),\n",
        "            output_type='pil'\n",
        "        )[0]\n",
        "        \n",
        "        all_results['cfg_sweep'][model_type].append((cfg, image))\n",
        "        print(f\"  ✓ CFG={cfg}\")\n",
        "    \n",
        "    # Test 2: Negative Prompt at Recommended CFG\n",
        "    print(f\"\\n2. Testing negative prompt effectiveness at CFG={config['recommended_cfg']}...\")\n",
        "    test_pipe.scheduler = test_scheduler\n",
        "    test_pipe.unet = test_unet\n",
        "    \n",
        "    # Without negative prompt\n",
        "    image_no_neg = test_pipe(\n",
        "        positive_prompt,\n",
        "        guidance_scale=config['recommended_cfg'],\n",
        "        num_inference_steps=config['steps'],\n",
        "        generator=torch.Generator().manual_seed(seed),\n",
        "        output_type='pil'\n",
        "    )[0]\n",
        "    \n",
        "    # With negative prompt\n",
        "    image_with_neg = test_pipe(\n",
        "        positive_prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        guidance_scale=config['recommended_cfg'],\n",
        "        num_inference_steps=config['steps'],\n",
        "        generator=torch.Generator().manual_seed(seed),\n",
        "        output_type='pil'\n",
        "    )[0]\n",
        "    \n",
        "    all_results['negative_test'][model_type]['without'] = image_no_neg\n",
        "    all_results['negative_test'][model_type]['with'] = image_with_neg\n",
        "    print(f\"  ✓ Generated both versions\")\n",
        "    \n",
        "    # Clean up\n",
        "    del test_pipe, test_base_unet, test_base_scheduler, test_unet, test_scheduler\n",
        "    if model_type != 'base':\n",
        "        del test_distilled_unet, test_distilled_scheduler\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Creating visualizations...\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "# Visualization 1: CFG Scale Comparison Grid (9 models x 5 CFG values)\n",
        "fig, axes = plt.subplots(len(model_types), len(cfg_scales), figsize=(25, len(model_types)*4), dpi=150)\n",
        "fig.suptitle(f'CFG Scale Response Across All Models\\nPrompt: \"{positive_prompt}\"', \n",
        "             fontsize=18, fontweight='bold', y=0.995)\n",
        "\n",
        "for model_idx, model_type in enumerate(model_types):\n",
        "    model_name = model_names[model_type]\n",
        "    rec_cfg = model_configs[model_type]['recommended_cfg']\n",
        "    \n",
        "    for cfg_idx, (cfg, img) in enumerate(all_results['cfg_sweep'][model_type]):\n",
        "        ax = axes[model_idx, cfg_idx]\n",
        "        ax.imshow(img)\n",
        "        \n",
        "        # Highlight recommended CFG with green border\n",
        "        if cfg == rec_cfg:\n",
        "            title = f'CFG={cfg}★'\n",
        "            ax.spines['bottom'].set_color('green')\n",
        "            ax.spines['top'].set_color('green')\n",
        "            ax.spines['left'].set_color('green')\n",
        "            ax.spines['right'].set_color('green')\n",
        "            ax.spines['bottom'].set_linewidth(4)\n",
        "            ax.spines['top'].set_linewidth(4)\n",
        "            ax.spines['left'].set_linewidth(4)\n",
        "            ax.spines['right'].set_linewidth(4)\n",
        "        else:\n",
        "            title = f'CFG={cfg}'\n",
        "        \n",
        "        ax.set_title(title, fontsize=11)\n",
        "        ax.axis('off')\n",
        "        \n",
        "        # Add model name on first column\n",
        "        if cfg_idx == 0:\n",
        "            ax.text(-0.1, 0.5, model_name, transform=ax.transAxes, \n",
        "                   fontsize=12, fontweight='bold', va='center', ha='right', rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "cfg_sweep_filename = f\"{output_dir}/all_models_cfg_sweep.png\"\n",
        "plt.savefig(cfg_sweep_filename, bbox_inches='tight', dpi=150)\n",
        "print(f\"✓ Saved CFG sweep: {cfg_sweep_filename}\")\n",
        "plt.show()\n",
        "\n",
        "# Visualization 2: Negative Prompt Effectiveness (9 models x 2 conditions)\n",
        "fig, axes = plt.subplots(len(model_types), 2, figsize=(12, len(model_types)*3), dpi=150)\n",
        "fig.suptitle(f'Negative Prompt Effectiveness Test\\nPositive: \"{positive_prompt}\"\\nNegative: \"{negative_prompt}\"', \n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "\n",
        "for model_idx, model_type in enumerate(model_types):\n",
        "    model_name = model_names[model_type]\n",
        "    rec_cfg = model_configs[model_type]['recommended_cfg']\n",
        "    \n",
        "    # Without negative\n",
        "    ax = axes[model_idx, 0]\n",
        "    ax.imshow(all_results['negative_test'][model_type]['without'])\n",
        "    ax.set_title(f'Without Negative\\n(CFG={rec_cfg})', fontsize=11)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    # With negative\n",
        "    ax = axes[model_idx, 1]\n",
        "    ax.imshow(all_results['negative_test'][model_type]['with'])\n",
        "    ax.set_title(f'With Negative\\n(CFG={rec_cfg})', fontsize=11)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    # Add model name\n",
        "    axes[model_idx, 0].text(-0.15, 0.5, model_name, transform=axes[model_idx, 0].transAxes, \n",
        "                           fontsize=12, fontweight='bold', va='center', ha='right', rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "negative_test_filename = f\"{output_dir}/all_models_negative_prompt_test.png\"\n",
        "plt.savefig(negative_test_filename, bbox_inches='tight', dpi=150)\n",
        "print(f\"✓ Saved negative prompt test: {negative_test_filename}\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✅ Comprehensive CFG and negative prompt testing complete!\")\n",
        "print(f\"\\nKey Observations to Look For:\")\n",
        "print(\"- Base SDXL: Should show strong CFG response and negative prompt adherence\")\n",
        "print(\"- DMD2/Turbo/Lightning: Minimal CFG response (optimized for CFG=0)\")\n",
        "print(\"- LCM: Slight CFG response at w∈[1,2]\")\n",
        "print(\"- Hyper-SDXL: Should maintain CFG response (CFG-preserved LoRA)\")\n",
        "print(\"- PCM/TCD: Designed for multi-step with CFG support\")\n",
        "print(\"- Flash: Adapter-based, check CFG/negative compatibility\")\n",
        "print(f\"\\nNegative prompt test: Look for absence of 'dog', 'blue', 'metal', 'standing'\")\n",
        "print(f\"                      Should preserve 'cat', 'red hat', 'wooden chair', 'sitting'\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Configuration - CFG Range Testing Only\n",
        "positive_prompt = \"one blue cube, one red cube, one green sphere\"\n",
        "seed = 42  # Fixed seed for consistency\n",
        "\n",
        "# CFG scales to test\n",
        "cfg_scales = [0.0, 1.0, 2.0, 3.0, 5.0, 7.0, 10.0]  # Extended range for better analysis\n",
        "\n",
        "# All 8 distillation models plus base\n",
        "model_types = ['base', 'dmd', 'turbo', 'lightning', 'lcm', 'hyper', 'pcm', 'tcd', 'flash']\n",
        "model_names = {\n",
        "    'base': 'Base SDXL',\n",
        "    'dmd': 'DMD2',\n",
        "    'turbo': 'SDXL-Turbo',\n",
        "    'lightning': 'SDXL-Lightning',\n",
        "    'lcm': 'LCM-LoRA',\n",
        "    'hyper': 'Hyper-SDXL',\n",
        "    'pcm': 'PCM',\n",
        "    'tcd': 'TCD',\n",
        "    'flash': 'Flash Diffusion'\n",
        "}\n",
        "\n",
        "# Recommended settings per model\n",
        "model_configs = {\n",
        "    'base': {'steps': 100, 'recommended_cfg': 5.0},\n",
        "    'dmd': {'steps': 4, 'recommended_cfg': 0.0},\n",
        "    'turbo': {'steps': 4, 'recommended_cfg': 0.0},\n",
        "    'lightning': {'steps': 4, 'recommended_cfg': 0.0},\n",
        "    'lcm': {'steps': 4, 'recommended_cfg': 1.0},\n",
        "    'hyper': {'steps': 8, 'recommended_cfg': 5.0},  # CFG-preserved version\n",
        "    'pcm': {'steps': 4, 'recommended_cfg': 2.0},\n",
        "    'tcd': {'steps': 4, 'recommended_cfg': 3.0},  # Supports standard CFG\n",
        "    'flash': {'steps': 4, 'recommended_cfg': 2.0}\n",
        "}\n",
        "\n",
        "output_dir = \"cfg-range-test\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Store results\n",
        "all_results = {\n",
        "    'cfg_sweep': {}  # CFG scale responses only\n",
        "}\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"CFG RANGE TESTING ACROSS ALL MODELS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Positive Prompt: {positive_prompt}\")\n",
        "print(f\"Seed: {seed}\")\n",
        "print(f\"CFG Scales: {cfg_scales}\")\n",
        "print(f\"Testing {len(model_types)} models\\n\")\n",
        "\n",
        "# Test each model\n",
        "for model_type in model_types:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Testing {model_names[model_type]} model...\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Load the model\n",
        "    if model_type == 'base':\n",
        "        # Base model doesn't need distillation loading\n",
        "        test_pipe, test_base_unet, test_base_scheduler, _, _ = load_sdxl_models(\n",
        "            distillation_type='dmd',  # Dummy, we'll use base components\n",
        "            weights_dtype=torch.bfloat16,\n",
        "            device=device\n",
        "        )\n",
        "        test_unet = test_base_unet\n",
        "        test_scheduler = test_base_scheduler\n",
        "    else:\n",
        "        test_pipe, test_base_unet, test_base_scheduler, test_distilled_unet, test_distilled_scheduler = load_sdxl_models(\n",
        "            distillation_type=model_type,\n",
        "            weights_dtype=torch.bfloat16,\n",
        "            device=device\n",
        "        )\n",
        "        test_unet = test_distilled_unet\n",
        "        test_scheduler = test_distilled_scheduler\n",
        "    \n",
        "    test_pipe.set_progress_bar_config(disable=True)\n",
        "    config = model_configs[model_type]\n",
        "    \n",
        "    all_results['cfg_sweep'][model_type] = []\n",
        "    \n",
        "    # CFG Scale Sweep\n",
        "    print(f\"Testing CFG scale response (recommended: {config['recommended_cfg']})...\")\n",
        "    for cfg in cfg_scales:\n",
        "        test_pipe.scheduler = test_scheduler\n",
        "        test_pipe.unet = test_unet\n",
        "        \n",
        "        image = test_pipe(\n",
        "            positive_prompt,\n",
        "            guidance_scale=cfg,\n",
        "            num_inference_steps=config['steps'],\n",
        "            generator=torch.Generator().manual_seed(seed),\n",
        "            output_type='pil'\n",
        "        )[0]\n",
        "        \n",
        "        all_results['cfg_sweep'][model_type].append((cfg, image))\n",
        "        print(f\"  ✓ CFG={cfg}\")\n",
        "    \n",
        "    # Clean up\n",
        "    del test_pipe, test_base_unet, test_base_scheduler, test_unet, test_scheduler\n",
        "    if model_type != 'base':\n",
        "        del test_distilled_unet, test_distilled_scheduler\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Creating CFG range visualization...\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "# Visualization: CFG Scale Comparison Grid (9 models x 7 CFG values)\n",
        "fig, axes = plt.subplots(len(model_types), len(cfg_scales), figsize=(30, len(model_types)*4), dpi=150)\n",
        "fig.suptitle(f'CFG Scale Response Across All Models\\nPrompt: \"{positive_prompt}\"', \n",
        "             fontsize=18, fontweight='bold', y=0.995)\n",
        "\n",
        "for model_idx, model_type in enumerate(model_types):\n",
        "    model_name = model_names[model_type]\n",
        "    rec_cfg = model_configs[model_type]['recommended_cfg']\n",
        "    \n",
        "    for cfg_idx, (cfg, img) in enumerate(all_results['cfg_sweep'][model_type]):\n",
        "        ax = axes[model_idx, cfg_idx]\n",
        "        ax.imshow(img)\n",
        "        \n",
        "        # Highlight recommended CFG with green border\n",
        "        if cfg == rec_cfg:\n",
        "            title = f'CFG={cfg}★'\n",
        "            ax.spines['bottom'].set_color('green')\n",
        "            ax.spines['top'].set_color('green')\n",
        "            ax.spines['left'].set_color('green')\n",
        "            ax.spines['right'].set_color('green')\n",
        "            ax.spines['bottom'].set_linewidth(4)\n",
        "            ax.spines['top'].set_linewidth(4)\n",
        "            ax.spines['left'].set_linewidth(4)\n",
        "            ax.spines['right'].set_linewidth(4)\n",
        "        else:\n",
        "            title = f'CFG={cfg}'\n",
        "        \n",
        "        ax.set_title(title, fontsize=11)\n",
        "        ax.axis('off')\n",
        "        \n",
        "        # Add model name on first column\n",
        "        if cfg_idx == 0:\n",
        "            ax.text(-0.1, 0.5, model_name, transform=ax.transAxes, \n",
        "                   fontsize=12, fontweight='bold', va='center', ha='right', rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "cfg_sweep_filename = f\"{output_dir}/cfg_range_comparison.png\"\n",
        "plt.savefig(cfg_sweep_filename, bbox_inches='tight', dpi=150)\n",
        "print(f\"✓ Saved CFG range comparison: {cfg_sweep_filename}\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✅ CFG range testing complete!\")\n",
        "print(f\"\\nKey Observations to Look For:\")\n",
        "print(\"- Base SDXL: Should show strong CFG response and prompt adherence\")\n",
        "print(\"- DMD2/Turbo/Lightning: Minimal CFG response (optimized for CFG=0)\")\n",
        "print(\"- LCM: Slight CFG response at w∈[1,2]\")\n",
        "print(\"- Hyper-SDXL: Should maintain CFG response (CFG-preserved LoRA)\")\n",
        "print(\"- PCM/TCD: Designed for multi-step with CFG support\")\n",
        "print(\"- Flash: Adapter-based, check CFG compatibility\")\n",
        "print(f\"\\nLook for changes in image composition, style, and adherence to prompt as CFG increases\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Configuration - Diversity Distillation with CFG in Base Step\n",
        "positive_prompt = \"a blue cube next to a red cube next to a green sphere\"\n",
        "seed = 42  # Fixed seed for consistency\n",
        "\n",
        "# CFG scales to test in base step (optimal range for base model)\n",
        "cfg_scales = [0.0, 1.0, 2.0, 3.0, 5.0, 7.0, 10.0]\n",
        "\n",
        "output_dir = \"diversity-cfg-test\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Store results\n",
        "diversity_results = []\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DIVERSITY DISTILLATION - CFG IN BASE STEP ONLY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Positive Prompt: {positive_prompt}\")\n",
        "print(f\"Seed: {seed}\")\n",
        "print(f\"CFG Scales (applied in base step): {cfg_scales}\")\n",
        "print(\"Using DMD distillation model\\n\")\n",
        "\n",
        "# Load the model\n",
        "print(\"Loading DMD model...\")\n",
        "pipe, base_unet, base_scheduler, distilled_unet, distilled_scheduler = load_sdxl_models(\n",
        "    distillation_type='dmd',\n",
        "    weights_dtype=torch.bfloat16,\n",
        "    device=device\n",
        ")\n",
        "pipe.set_progress_bar_config(disable=True)\n",
        "\n",
        "# Test diversity distillation with different CFG in base step\n",
        "print(\"Testing diversity distillation with CFG applied only in base step...\")\n",
        "for cfg in cfg_scales:\n",
        "    # Use diversity_distillation function with CFG in base step, distilled step uses CFG=0\n",
        "    images = diversity_distillation(\n",
        "        prompt=positive_prompt,\n",
        "        seed=seed,\n",
        "        pipe=pipe,\n",
        "        base_unet=base_unet,\n",
        "        distilled_unet=distilled_unet,\n",
        "        distilled_scheduler=distilled_scheduler,\n",
        "        base_guidance_scale=cfg,  # Apply CFG in base step\n",
        "        distilled_guidance_scale=0,  # Keep distilled at CFG=0\n",
        "        num_inference_steps=4,\n",
        "        run_base_till=1\n",
        "    )\n",
        "    \n",
        "    diversity_results.append((cfg, images[0]))\n",
        "    print(f\"  ✓ CFG={cfg} in base step\")\n",
        "\n",
        "# Clean up\n",
        "del pipe, base_unet, base_scheduler, distilled_unet, distilled_scheduler\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Creating diversity CFG visualization...\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "# Visualization: Diversity Distillation CFG in Base Step\n",
        "fig, axes = plt.subplots(1, len(cfg_scales), figsize=(25, 4), dpi=150)\n",
        "fig.suptitle(f'Diversity Distillation - CFG Applied Only in Base Step\\nPrompt: \"{positive_prompt}\"\\n(Base 1 step with CFG + Distilled 3 steps with CFG=0)', \n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "\n",
        "for cfg_idx, (cfg, img) in enumerate(diversity_results):\n",
        "    ax = axes[cfg_idx]\n",
        "    ax.imshow(img)\n",
        "    \n",
        "    # Highlight optimal CFG (5.0) with green border\n",
        "    if cfg == 5.0:\n",
        "        title = f'CFG={cfg}★\\n(Base)'\n",
        "        ax.spines['bottom'].set_color('green')\n",
        "        ax.spines['top'].set_color('green')\n",
        "        ax.spines['left'].set_color('green')\n",
        "        ax.spines['right'].set_color('green')\n",
        "        ax.spines['bottom'].set_linewidth(4)\n",
        "        ax.spines['top'].set_linewidth(4)\n",
        "        ax.spines['left'].set_linewidth(4)\n",
        "        ax.spines['right'].set_linewidth(4)\n",
        "    else:\n",
        "        title = f'CFG={cfg}\\n(Base)'\n",
        "    \n",
        "    ax.set_title(title, fontsize=12)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "diversity_filename = f\"{output_dir}/diversity_cfg_in_base.png\"\n",
        "plt.savefig(diversity_filename, bbox_inches='tight', dpi=150)\n",
        "print(f\"✓ Saved diversity CFG test: {diversity_filename}\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✅ Diversity distillation CFG testing complete!\")\n",
        "print(\"Note: CFG is applied only in the base model's first step, distilled model uses CFG=0.\")\n",
        "print(\"Optimal CFG=5.0 is highlighted - observe how base step CFG affects final diversity output.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Configuration - Diversity Distillation with CFG in Base Step - ALL MODELS\n",
        "positive_prompt = \"v\"\n",
        "seed = 42  # Fixed seed for consistency\n",
        "\n",
        "# CFG scales to test in base step\n",
        "cfg_scales = [0.0, 1.0, 2.0, 3.0, 5.0, 7.0, 10.0]\n",
        "\n",
        "# All distillation models to test\n",
        "model_types = ['dmd', 'turbo', 'lightning', 'lcm', 'hyper', 'pcm', 'tcd', 'flash']\n",
        "model_names = {\n",
        "    'dmd': 'DMD2',\n",
        "    'turbo': 'SDXL-Turbo',\n",
        "    'lightning': 'SDXL-Lightning',\n",
        "    'lcm': 'LCM-LoRA',\n",
        "    'hyper': 'Hyper-SDXL',\n",
        "    'pcm': 'PCM',\n",
        "    'tcd': 'TCD',\n",
        "    'flash': 'Flash Diffusion'\n",
        "}\n",
        "\n",
        "# Recommended CFG settings per model\n",
        "model_configs = {\n",
        "    'dmd': {'recommended_cfg': 0.0},\n",
        "    'turbo': {'recommended_cfg': 0.0},\n",
        "    'lightning': {'recommended_cfg': 0.0},\n",
        "    'lcm': {'recommended_cfg': 1.0},\n",
        "    'hyper': {'recommended_cfg': 5.0},\n",
        "    'pcm': {'recommended_cfg': 2.0},\n",
        "    'tcd': {'recommended_cfg': 3.0},\n",
        "    'flash': {'recommended_cfg': 2.0}\n",
        "}\n",
        "\n",
        "output_dir = \"diversity-cfg-test-all-models\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Store results: {model_type: [(cfg, image), ...]}\n",
        "all_diversity_results = {}\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DIVERSITY DISTILLATION - CFG SWEEP ACROSS ALL MODELS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Positive Prompt: {positive_prompt}\")\n",
        "print(f\"Seed: {seed}\")\n",
        "print(f\"CFG Scales (applied in base step): {cfg_scales}\")\n",
        "print(f\"Testing {len(model_types)} distillation models\\n\")\n",
        "\n",
        "# Test each model\n",
        "for model_type in model_types:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Testing {model_names[model_type]} model...\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Load the model\n",
        "    pipe, base_unet, base_scheduler, distilled_unet, distilled_scheduler = load_sdxl_models(\n",
        "        distillation_type=model_type,\n",
        "        weights_dtype=torch.bfloat16,\n",
        "        device=device\n",
        "    )\n",
        "    pipe.set_progress_bar_config(disable=True)\n",
        "    \n",
        "    all_diversity_results[model_type] = []\n",
        "    rec_cfg = model_configs[model_type]['recommended_cfg']\n",
        "    \n",
        "    # Test diversity distillation with different CFG in base step\n",
        "    print(f\"CFG sweep (recommended: {rec_cfg})...\")\n",
        "    for cfg in cfg_scales:\n",
        "        # Use diversity_distillation function with CFG in base step, distilled step uses CFG=0\n",
        "        images = diversity_distillation(\n",
        "            prompt=positive_prompt,\n",
        "            seed=seed,\n",
        "            pipe=pipe,\n",
        "            base_unet=base_unet,\n",
        "            distilled_unet=distilled_unet,\n",
        "            distilled_scheduler=distilled_scheduler,\n",
        "            base_guidance_scale=cfg,  # Apply CFG in base step\n",
        "            distilled_guidance_scale=rec_cfg,  # Keep distilled at CFG=0\n",
        "            num_inference_steps=4,\n",
        "            run_base_till=1\n",
        "        )\n",
        "        \n",
        "        all_diversity_results[model_type].append((cfg, images[0]))\n",
        "        print(f\"  ✓ CFG={cfg}\")\n",
        "    \n",
        "    # Clean up\n",
        "    del pipe, base_unet, base_scheduler, distilled_unet, distilled_scheduler\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Creating multi-model diversity CFG visualization...\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "# Visualization: All Models x CFG Scales (8 models x 7 CFG values)\n",
        "fig, axes = plt.subplots(len(model_types), len(cfg_scales), figsize=(30, len(model_types)*4), dpi=150)\n",
        "fig.suptitle(f'Diversity Distillation - CFG Sweep Across All Models\\nPrompt: \"{positive_prompt}\"\\n(Base 1 step with CFG + Distilled 3 steps with CFG=0)', \n",
        "             fontsize=18, fontweight='bold', y=0.995)\n",
        "\n",
        "for model_idx, model_type in enumerate(model_types):\n",
        "    model_name = model_names[model_type]\n",
        "    rec_cfg = model_configs[model_type]['recommended_cfg']\n",
        "    \n",
        "    for cfg_idx, (cfg, img) in enumerate(all_diversity_results[model_type]):\n",
        "        ax = axes[model_idx, cfg_idx]\n",
        "        ax.imshow(img)\n",
        "        \n",
        "        # Highlight recommended CFG with green border\n",
        "        if cfg == rec_cfg:\n",
        "            title = f'CFG={cfg}★'\n",
        "            ax.spines['bottom'].set_color('green')\n",
        "            ax.spines['top'].set_color('green')\n",
        "            ax.spines['left'].set_color('green')\n",
        "            ax.spines['right'].set_color('green')\n",
        "            ax.spines['bottom'].set_linewidth(4)\n",
        "            ax.spines['top'].set_linewidth(4)\n",
        "            ax.spines['left'].set_linewidth(4)\n",
        "            ax.spines['right'].set_linewidth(4)\n",
        "        else:\n",
        "            title = f'CFG={cfg}'\n",
        "        \n",
        "        ax.set_title(title, fontsize=11)\n",
        "        ax.axis('off')\n",
        "        \n",
        "        # Add model name on first column\n",
        "        if cfg_idx == 0:\n",
        "            ax.text(-0.12, 0.5, model_name, transform=ax.transAxes, \n",
        "                   fontsize=12, fontweight='bold', va='center', ha='right', rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "diversity_filename = f\"{output_dir}/diversity_cfg_all_models.png\"\n",
        "plt.savefig(diversity_filename, bbox_inches='tight', dpi=150)\n",
        "print(f\"✓ Saved multi-model diversity CFG test: {diversity_filename}\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✅ Multi-model diversity distillation CFG testing complete!\")\n",
        "print(\"Note: CFG is applied only in the base model's first step, distilled model uses CFG=0.\")\n",
        "print(\"Green borders highlight the recommended CFG for each model.\")\n",
        "print(f\"Output saved to: {output_dir}/\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Configuration - Diversity Distillation with CFG in Base Step - ALL MODELS\n",
        "positive_prompt = \"horse pulling a carriage\"\n",
        "seed = 42  # Fixed seed for consistency\n",
        "\n",
        "# CFG scales to test in base step\n",
        "cfg_scales = [0.0, 1.0, 2.0, 3.0, 5.0, 7.0, 10.0]\n",
        "\n",
        "# All distillation models to test\n",
        "model_types = ['dmd', 'turbo', 'lightning', 'lcm', 'hyper', 'pcm', 'tcd', 'flash']\n",
        "model_names = {\n",
        "    'dmd': 'DMD2',\n",
        "    'turbo': 'SDXL-Turbo',\n",
        "    'lightning': 'SDXL-Lightning',\n",
        "    'lcm': 'LCM-LoRA',\n",
        "    'hyper': 'Hyper-SDXL',\n",
        "    'pcm': 'PCM',\n",
        "    'tcd': 'TCD',\n",
        "    'flash': 'Flash Diffusion'\n",
        "}\n",
        "\n",
        "# Recommended CFG settings per model\n",
        "model_configs = {\n",
        "    'dmd': {'recommended_cfg': 0.0},\n",
        "    'turbo': {'recommended_cfg': 0.0},\n",
        "    'lightning': {'recommended_cfg': 0.0},\n",
        "    'lcm': {'recommended_cfg': 1.0},\n",
        "    'hyper': {'recommended_cfg': 5.0},\n",
        "    'pcm': {'recommended_cfg': 2.0},\n",
        "    'tcd': {'recommended_cfg': 3.0},\n",
        "    'flash': {'recommended_cfg': 2.0}\n",
        "}\n",
        "\n",
        "output_dir = \"diversity-cfg-test-all-models\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Store results: {model_type: [(cfg, image), ...]}\n",
        "all_diversity_results = {}\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DIVERSITY DISTILLATION - CFG SWEEP ACROSS ALL MODELS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Positive Prompt: {positive_prompt}\")\n",
        "print(f\"Seed: {seed}\")\n",
        "print(f\"CFG Scales (applied in base step): {cfg_scales}\")\n",
        "print(f\"Testing {len(model_types)} distillation models\\n\")\n",
        "\n",
        "# Test each model\n",
        "for model_type in model_types:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Testing {model_names[model_type]} model...\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Load the model\n",
        "    pipe, base_unet, base_scheduler, distilled_unet, distilled_scheduler = load_sdxl_models(\n",
        "        distillation_type=model_type,\n",
        "        weights_dtype=torch.bfloat16,\n",
        "        device=device\n",
        "    )\n",
        "    pipe.set_progress_bar_config(disable=True)\n",
        "    \n",
        "    all_diversity_results[model_type] = []\n",
        "    rec_cfg = model_configs[model_type]['recommended_cfg']\n",
        "    \n",
        "    # Test diversity distillation with different CFG in base step\n",
        "    print(f\"CFG sweep (recommended: {rec_cfg})...\")\n",
        "    for cfg in cfg_scales:\n",
        "        # Use diversity_distillation function with CFG in base step, distilled step uses CFG=0\n",
        "        images = diversity_distillation(\n",
        "            prompt=positive_prompt,\n",
        "            seed=seed,\n",
        "            pipe=pipe,\n",
        "            base_unet=base_unet,\n",
        "            distilled_unet=distilled_unet,\n",
        "            distilled_scheduler=distilled_scheduler,\n",
        "            base_guidance_scale=cfg,  # Apply CFG in base step\n",
        "            distilled_guidance_scale=cfg,  # Keep distilled at CFG=0\n",
        "            num_inference_steps=4,\n",
        "            run_base_till=1\n",
        "        )\n",
        "        \n",
        "        all_diversity_results[model_type].append((cfg, images[0]))\n",
        "        print(f\"  ✓ CFG={cfg}\")\n",
        "    \n",
        "    # Clean up\n",
        "    del pipe, base_unet, base_scheduler, distilled_unet, distilled_scheduler\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Creating multi-model diversity CFG visualization...\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "# Visualization: All Models x CFG Scales (8 models x 7 CFG values)\n",
        "fig, axes = plt.subplots(len(model_types), len(cfg_scales), figsize=(30, len(model_types)*4), dpi=150)\n",
        "fig.suptitle(f'Diversity Distillation - CFG Sweep Across All Models\\nPrompt: \"{positive_prompt}\"\\n(Base 1 step with CFG + Distilled 3 steps with CFG=0)', \n",
        "             fontsize=18, fontweight='bold', y=0.995)\n",
        "\n",
        "for model_idx, model_type in enumerate(model_types):\n",
        "    model_name = model_names[model_type]\n",
        "    rec_cfg = model_configs[model_type]['recommended_cfg']\n",
        "    \n",
        "    for cfg_idx, (cfg, img) in enumerate(all_diversity_results[model_type]):\n",
        "        ax = axes[model_idx, cfg_idx]\n",
        "        ax.imshow(img)\n",
        "        \n",
        "        # Highlight recommended CFG with green border\n",
        "        if cfg == rec_cfg:\n",
        "            title = f'CFG={cfg}★'\n",
        "            ax.spines['bottom'].set_color('green')\n",
        "            ax.spines['top'].set_color('green')\n",
        "            ax.spines['left'].set_color('green')\n",
        "            ax.spines['right'].set_color('green')\n",
        "            ax.spines['bottom'].set_linewidth(4)\n",
        "            ax.spines['top'].set_linewidth(4)\n",
        "            ax.spines['left'].set_linewidth(4)\n",
        "            ax.spines['right'].set_linewidth(4)\n",
        "        else:\n",
        "            title = f'CFG={cfg}'\n",
        "        \n",
        "        ax.set_title(title, fontsize=11)\n",
        "        ax.axis('off')\n",
        "        \n",
        "        # Add model name on first column\n",
        "        if cfg_idx == 0:\n",
        "            ax.text(-0.12, 0.5, model_name, transform=ax.transAxes, \n",
        "                   fontsize=12, fontweight='bold', va='center', ha='right', rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "diversity_filename = f\"{output_dir}/diversity_cfg_all_models.png\"\n",
        "plt.savefig(diversity_filename, bbox_inches='tight', dpi=150)\n",
        "print(f\"✓ Saved multi-model diversity CFG test: {diversity_filename}\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✅ Multi-model diversity distillation CFG testing complete!\")\n",
        "print(\"Note: CFG is applied only in the base model's first step, distilled model uses CFG=0.\")\n",
        "print(\"Green borders highlight the recommended CFG for each model.\")\n",
        "print(f\"Output saved to: {output_dir}/\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}