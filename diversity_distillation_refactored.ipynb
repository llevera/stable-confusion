{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5cf0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.enable_grad(False)\n",
    "\n",
    "# Enable PyTorch performance optimizations\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('../.')\n",
    "from utils.load_util import load_model, load_pipe\n",
    "\n",
    "distillation_type = 'dmd'  # what type of distillation model do you want to use (\"dmd\", \"lcm\", \"turbo\", \"lightning\")\n",
    "device = 'cuda:0'  # Use CUDA for A100 GPU\n",
    "weights_dtype = torch.bfloat16  # Use bfloat16 for better performance on A100\n",
    "\n",
    "# Load model with proper handling of distillation_type=None\n",
    "result = load_model(distillation_type=distillation_type, \n",
    "                    weights_dtype=weights_dtype, \n",
    "                    device=device)\n",
    "\n",
    "if distillation_type is None:\n",
    "    pipe, base_unet, base_scheduler = result\n",
    "    distilled_unet = None\n",
    "    distilled_scheduler = None\n",
    "else:\n",
    "    pipe, base_unet, base_scheduler, distilled_unet, distilled_scheduler = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2530da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diversity_distillation(prompt, seed, pipe, base_unet, distilled_unet, distilled_scheduler, base_guidance_scale=5, distilled_guidance_scale=0, num_inference_steps=4, run_base_till=1):\n",
    "    \"\"\"Generate images using diversity distillation (base + distilled model).\"\"\"\n",
    "    pipe.scheduler = distilled_scheduler\n",
    "    pipe.unet = base_unet\n",
    "\n",
    "    base_latents = pipe(prompt,\n",
    "                    guidance_scale=base_guidance_scale,\n",
    "                    till_timestep=run_base_till, \n",
    "                    num_inference_steps=num_inference_steps,\n",
    "                    generator=torch.Generator().manual_seed(seed),\n",
    "                    output_type='latent'\n",
    "                   )\n",
    "\n",
    "    if distilled_unet is not None:\n",
    "        pipe.unet = distilled_unet\n",
    "    \n",
    "    images = pipe(prompt,\n",
    "                 guidance_scale=distilled_guidance_scale,\n",
    "                 start_latents = base_latents,   \n",
    "                 num_inference_steps=num_inference_steps,\n",
    "                 from_timestep=run_base_till,\n",
    "                 output_type='pil'\n",
    "                )\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042c9643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def distilled_only_generation(prompt, seed, pipe, distilled_unet, distilled_scheduler, guidance_scale=0, num_inference_steps=4):\n",
    "    \"\"\"Generate images using only the distilled model.\"\"\"\n",
    "    if distilled_unet is None:\n",
    "        raise ValueError(\"No distilled model loaded. Cannot use distilled_only_generation.\")\n",
    "    \n",
    "    pipe.scheduler = distilled_scheduler\n",
    "    pipe.unet = distilled_unet\n",
    "    \n",
    "    images = pipe(prompt,\n",
    "                 guidance_scale=guidance_scale,\n",
    "                 num_inference_steps=num_inference_steps,\n",
    "                 generator=torch.Generator().manual_seed(seed),\n",
    "                 output_type='pil'\n",
    "                )\n",
    "    return images\n",
    "\n",
    "def base_only_generation(prompt, seed, pipe, base_unet, base_scheduler, guidance_scale=5, num_inference_steps=20):\n",
    "    \"\"\"Generate images using only the base model.\"\"\"\n",
    "    pipe.scheduler = base_scheduler\n",
    "    pipe.unet = base_unet\n",
    "    \n",
    "    images = pipe(prompt,\n",
    "                 guidance_scale=guidance_scale,\n",
    "                 num_inference_steps=num_inference_steps,\n",
    "                 generator=torch.Generator().manual_seed(seed),\n",
    "                 output_type='pil'\n",
    "                )\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8a6c0e",
   "metadata": {},
   "source": [
    "# Multi-Model Comparison with load_model\n",
    "\n",
    "This notebook uses the refactored `load_model` function where `distilled_model = None` when no distillation model is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c95560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "prompt = \"frog in a hat\"\n",
    "num_images = 3  # Reduce for testing\n",
    "nrows = 1\n",
    "ncols = 3\n",
    "\n",
    "# Models to compare\n",
    "model_types = ['dmd', 'lightning', 'turbo']\n",
    "model_names = {\n",
    "    'dmd': 'DMD',\n",
    "    'lightning': 'Lightning', \n",
    "    'turbo': 'Turbo'\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"multi_model_comparison\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Store results for each model and method\n",
    "all_results = {}\n",
    "timing_results = {}\n",
    "\n",
    "print(f\"Comparing 3 generation methods across {len(model_types)} models: {', '.join(model_names.values())}\")\n",
    "print(f\"Generating {num_images} images per method per model with prompt: '{prompt}'\\n\")\n",
    "\n",
    "# Load and test each model\n",
    "for model_type in model_types:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Loading {model_names[model_type]} model...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load the model using load_model\n",
    "    start_load = time.time()\n",
    "    result = load_model(\n",
    "        distillation_type=model_type,\n",
    "        weights_dtype=torch.bfloat16,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Unpack result - distilled model should not be None for these model types\n",
    "    model_pipe, model_base_unet, model_base_scheduler, model_distilled_unet, model_distilled_scheduler = result\n",
    "    load_time = time.time() - start_load\n",
    "    print(f\"✓ {model_names[model_type]} loaded in {load_time:.2f}s\")\n",
    "    \n",
    "    # Initialize storage for this model\n",
    "    all_results[model_type] = {\n",
    "        'diversity': [],\n",
    "        'distilled': [],\n",
    "        'base': []\n",
    "    }\n",
    "    timing_results[model_type] = {\n",
    "        'diversity': {'total': 0, 'average': 0},\n",
    "        'distilled': {'total': 0, 'average': 0},\n",
    "        'base': {'total': 0, 'average': 0},\n",
    "        'load_time': load_time\n",
    "    }\n",
    "    \n",
    "    model_pipe.set_progress_bar_config(disable=True)\n",
    "    \n",
    "    # Generate images with all three methods\n",
    "    for i in tqdm(range(num_images), desc=f\"Generating {model_names[model_type]} images\"):\n",
    "        seed = np.random.randint(0, 2**32 - 1)\n",
    "        \n",
    "        # 1. Diversity Distillation (Base + Distilled)\n",
    "        start_time = time.perf_counter()\n",
    "        diversity_images = diversity_distillation(\n",
    "            prompt, seed, model_pipe, \n",
    "            model_base_unet, model_distilled_unet, \n",
    "            model_distilled_scheduler\n",
    "        )\n",
    "        gen_time = time.perf_counter() - start_time\n",
    "        timing_results[model_type]['diversity']['total'] += gen_time\n",
    "        all_results[model_type]['diversity'].append(diversity_images[0])\n",
    "        \n",
    "        # 2. Distilled Model Only\n",
    "        start_time = time.perf_counter()\n",
    "        distilled_images = distilled_only_generation(\n",
    "            prompt, seed, model_pipe,\n",
    "            model_distilled_unet, model_distilled_scheduler\n",
    "        )\n",
    "        gen_time = time.perf_counter() - start_time\n",
    "        timing_results[model_type]['distilled']['total'] += gen_time\n",
    "        all_results[model_type]['distilled'].append(distilled_images[0])\n",
    "        \n",
    "        # 3. Base Model Only\n",
    "        start_time = time.perf_counter()\n",
    "        base_images = base_only_generation(\n",
    "            prompt, seed, model_pipe,\n",
    "            model_base_unet, model_base_scheduler\n",
    "        )\n",
    "        gen_time = time.perf_counter() - start_time\n",
    "        timing_results[model_type]['base']['total'] += gen_time\n",
    "        all_results[model_type]['base'].append(base_images[0])\n",
    "    \n",
    "    # Calculate averages\n",
    "    for method in ['diversity', 'distilled', 'base']:\n",
    "        timing_results[model_type][method]['average'] = timing_results[model_type][method]['total'] / num_images\n",
    "    \n",
    "    print(f\"✓ Generated {num_images*3} images total ({num_images} per method)\")\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    del model_pipe, model_base_unet, model_base_scheduler, model_distilled_unet, model_distilled_scheduler\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"✅ Multi-model comparison complete!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3bf578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading with distillation_type=None (base model only)\n",
    "print(\"Testing load_model with distillation_type=None...\\n\")\n",
    "\n",
    "result_base = load_model(\n",
    "    distillation_type=None,\n",
    "    weights_dtype=torch.bfloat16,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# When distillation_type=None, only 3 values are returned\n",
    "base_pipe, base_model_unet, base_model_scheduler = result_base\n",
    "distilled_unet_test = None\n",
    "distilled_scheduler_test = None\n",
    "\n",
    "print(f\"✓ Base model loaded successfully\")\n",
    "print(f\"  pipe: {type(base_pipe).__name__}\")\n",
    "print(f\"  base_unet: {type(base_model_unet).__name__}\")\n",
    "print(f\"  base_scheduler: {type(base_model_scheduler).__name__}\")\n",
    "print(f\"  distilled_unet: {distilled_unet_test}\")\n",
    "print(f\"  distilled_scheduler: {distilled_scheduler_test}\")\n",
    "\n",
    "# Test that diversity_distillation still works with None distilled model\n",
    "test_prompt = \"a cat on a table\"\n",
    "test_seed = 42\n",
    "\n",
    "try:\n",
    "    test_images = diversity_distillation(\n",
    "        test_prompt, test_seed, base_pipe, \n",
    "        base_model_unet, distilled_unet_test, \n",
    "        distilled_scheduler_test\n",
    "    )\n",
    "    print(f\"\\n✓ diversity_distillation works with distilled_unet=None\")\n",
    "    print(f\"  Generated {len(test_images)} image(s)\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Error: {e}\")\n",
    "\n",
    "# Cleanup\n",
    "del base_pipe, base_model_unet, base_model_scheduler\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n✅ Test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7454db75",
   "metadata": {},
   "source": [
    "## Summary of Refactoring\n",
    "\n",
    "### Key Changes:\n",
    "\n",
    "1. **Updated Imports**: Changed from `load_sdxl_models` to `load_model`\n",
    "   - The new `load_model` function is the correct one in `load_util.py`\n",
    "\n",
    "2. **Proper Return Value Handling**:\n",
    "   - When `distillation_type=None`: Returns only `(pipe, base_unet, base_scheduler)`\n",
    "   - When `distillation_type` is specified: Returns `(pipe, base_unet, base_scheduler, distilled_unet, distilled_scheduler)`\n",
    "\n",
    "3. **None Handling**:\n",
    "   - `distilled_unet = None` clearly indicates when no distillation model is loaded\n",
    "   - Functions check for `None` before using distilled models\n",
    "\n",
    "4. **Function Updates**:\n",
    "   - `diversity_distillation()`: Checks `if distilled_unet is not None` before using it\n",
    "   - `distilled_only_generation()`: Raises error if `distilled_unet` is None\n",
    "   - `base_only_generation()`: Always works since base model is always present\n",
    "\n",
    "5. **Fixed `load_util.py`**:\n",
    "   - Updated `load_pipe()` to use `load_model()` instead of non-existent `load_sdxl_models()`\n",
    "\n",
    "### Benefits:\n",
    "- ✅ Clear API: `distilled_unet = None` explicitly shows when no distillation model exists\n",
    "- ✅ Consistent: All model loading uses the same `load_model()` function\n",
    "- ✅ Flexible: Supports both base-only and base+distilled configurations\n",
    "- ✅ Safe: Functions validate model availability before use"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
